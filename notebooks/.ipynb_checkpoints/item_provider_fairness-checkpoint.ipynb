{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook #3: Investigation on Item Provider Fairness\n",
    "\n",
    "This notebook will consider the directors of movies in Movielens as the item providers and investigates how unfairness based on gender groups affects providers' group visibility and exposure with respect to their representation in the item catalog. We also showcase an umsampling strategy that upsamples interations involving items of a minority group to improve fairness. \n",
    "\n",
    "** While gender is by no means a binary construct, to the best of our knowledge no dataset with non-binary genders exists. What we are considering is a binary feature, as the current publicly available platforms offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the working environment for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/My Drive/bias-recsys-tutorial/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.train_test_splitter import *\n",
    "from helpers.instances_upsampler import *\n",
    "from models.pointwise import PointWise\n",
    "from models.pairwise import PairWise\n",
    "from models.mostpop import MostPop\n",
    "from models.random import Random\n",
    "from helpers.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis: representation of gender-based groups of providers in the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, given that we are considering a pre-processing approach, we will go through the whole experimental pipeline again. To this end, we will first load the Movielens 1M dataset ans inspect to what extent the differen groups are represented in the catalog.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ml1m'          \n",
    "method = 'utime_pfair'\n",
    "user_field = 'user_id'\n",
    "item_field = 'item_id'\n",
    "rating_field = 'rating'\n",
    "time_field = 'timestamp'\n",
    "type_field = 'type_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/datasets/' + dataset + '.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need also to append thegender information to the original dataset. We will leverage a csv that, for each item, gives the percentage of directors belonging to the two genders. Please note that more than one director can be associated to a movie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirgender = pd.read_csv('../data/datasets/' + dataset + '-dir-gender' + '.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirgender.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dirgender[dirgender['gender_1'] > 0]) / len(dirgender), len(dirgender[dirgender['gender_1'] == 0]) / len(dirgender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the first gender group (female group) represents a minority provider group in this dataset, with a representation of 5% in the catalog. The other gender group (male group) is reprensented by the 83% in the catalog. Summing up the two percentages, we do not reach a 100%. This is due to the fact that, for some movies, we were not able to get the gender information of the respective directors. For the sake of easiness, we assume that items of providers whose gender is unknown are part of the minority group, together with the female group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirgender['minority'] = dirgender['gender_1'].apply(lambda x: 1.0 if math.isnan(x) else x)\n",
    "dirgender['majority'] = dirgender['gender_2'].apply(lambda x: 0.0 if math.isnan(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dirgender['gender_1']\n",
    "del dirgender['gender_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dirgender[dirgender['minority'] > 0]) / len(dirgender), len(dirgender[dirgender['minority'] == 0]) / len(dirgender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_minority_rep = len(dirgender[dirgender['minority'] > 0]) / len(dirgender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, in this notebook, we will consider a minority group with a representation of 16% in the catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, dirgender, on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(n=10, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis: analysis of provider's group visibility and exposure in recommendations\n",
    "\n",
    "We will use the same cutoffs we have configured in the first notebook. \n",
    "\n",
    "**IMPORTANT BOOKMARK** Please bookmark this point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = np.array([5, 10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = ['utime_pairwise', 'utime_random', 'utime_mostpop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "for model_type in model_types:\n",
    "    metrics[model_type] = load_obj(os.path.join(data_path, 'outputs/metrics/' + dataset + '_' + model_type + '_metrics.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16.5})\n",
    "plt.figure(figsize=(30, 7.5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title(r'Precision')\n",
    "plt.xlabel('Cutoff Value')\n",
    "plt.ylabel('Precision')\n",
    "for model_type in model_types:\n",
    "    plt.plot(cutoffs, [np.mean(metrics[model_type]['precision'][k,:]) for k in range(len(cutoffs))], label=model_type)\n",
    "plt.xticks(cutoffs)\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title(r'Recall')\n",
    "plt.xlabel('Cutoff Value')\n",
    "plt.ylabel('Recall')\n",
    "for model_type in model_types:\n",
    "    plt.plot(cutoffs, [np.mean(metrics[model_type]['recall'][k,:]) for k in range(len(cutoffs))], label=model_type)\n",
    "plt.xticks(cutoffs)\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title(r'NDCG')\n",
    "plt.xlabel('Cutoff Value')\n",
    "plt.ylabel('NDCG')\n",
    "for model_type in model_types:\n",
    "    plt.plot(cutoffs, [np.mean(metrics[model_type]['ndcg'][k,:]) for k in range(len(cutoffs))], label=model_type)\n",
    "plt.xticks(cutoffs)\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16.5})\n",
    "plt.figure(figsize=(30, 7.5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(r'Disparate Visibility')\n",
    "plt.xlabel('Cutoff Value')\n",
    "plt.ylabel('Disparate Visibility')\n",
    "for model_type in model_types:\n",
    "    plt.plot(cutoffs, [abs(np.mean(metrics[model_type]['visibility'][k,:]) - original_minority_rep) for k in range(len(cutoffs))], label=model_type)\n",
    "plt.xticks(cutoffs)\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(r'Disparate Exposure')\n",
    "plt.xlabel('Cutoff Value')\n",
    "plt.ylabel('Disparate Exposure')\n",
    "for model_type in model_types:\n",
    "    plt.plot(cutoffs, [abs(np.mean(metrics[model_type]['exposure'][k,:]) - original_minority_rep) for k in range(len(cutoffs))], label=model_type)\n",
    "plt.xticks(cutoffs)\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample treatment to increase fairness between providers' groups: pre-processing\n",
    "\n",
    "This part will show how to improve fairness among provider groups by upsampling interactions involving the minority group of providers. This example is a didactic version of the work proposed by Boratto et al. (2020b). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we split again train and test data and prepare the data needed to initialize a recommendation model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smode = 'utime_pfair'\n",
    "train_ratio = 0.80        \n",
    "min_train_samples = 8\n",
    "min_test_samples = 2\n",
    "min_time = None\n",
    "max_time = None\n",
    "step_time = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if smode == 'uftime_pfair':\n",
    "    traintest = fixed_timestamp(data, min_train_samples, min_test_samples, min_time, max_time, step_time, user_field, item_field, time_field, rating_field)\n",
    "elif smode == 'utime_pfair':\n",
    "    traintest = user_timestamp(data, train_ratio, min_train_samples+min_test_samples, user_field, item_field, time_field)\n",
    "elif smode == 'urandom_pfair':\n",
    "    traintest = user_random(data, train_ratio, min_train_samples+min_test_samples, user_field, item_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = traintest[traintest['set']=='train'].copy()\n",
    "test = traintest[traintest['set']=='test'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train[train['minority'] > 0]) / len(train), len(train[train['minority'] == 0]) / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(np.unique(traintest[user_field].values))\n",
    "items = list(np.unique(traintest[item_field].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_metadata = traintest.drop_duplicates(subset=['item_id'], keep='first')\n",
    "category_per_item = items_metadata[type_field].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we identify the set of items belonging to the minority group and we run the upsampling strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_w_min = np.unique(traintest[traintest['minority'] > 0]['item_id'].values)\n",
    "items_w_maj = np.unique(traintest[traintest['minority'] == 0]['item_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_map = traintest.drop_duplicates(subset='item_id', keep='first')\n",
    "item_group = {i: (0.0 if v > 0 else 1.0) for i, v in zip(items_map['item_id'].values, items_map['minority'].values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umode = 'fake'\n",
    "utarget = 0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if umode == 'real':\n",
    "    train = real(train, 'minority', target=utarget)\n",
    "if umode == 'fake':\n",
    "    train = fake(train, 'minority', items_w_min, target=utarget)\n",
    "if umode == 'fakebypop':\n",
    "    train = fakeByPop(train, 'minority', items_w_min, target=utarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sample(n=10, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we upsampled the representation of the minority group in the interactions, reaching 20%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['rating'] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run another instance of the pairwise algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'pairwise'\n",
    "model = PairWise(users, items, train, test, category_per_item, item_field, user_field, rating_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(no_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.get_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(predictions, os.path.join(data_path, 'outputs/predictions/' + dataset + '_' + smode + '_' + model_type + '_scores.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to specify how items are associated to provider groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(item_group, os.path.join(data_path, 'datasets', 'ml1m-item-group'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(item_group=item_group, cutoffs=cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(metrics, os.path.join(data_path, 'outputs/metrics/' + dataset + '_' + smode + '_' + model_type + '_metrics.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_metrics(index_k=int(np.where(cutoffs == 10)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we come back to the **IMPORTANT BOOKMARK** mentioned above, using cutoffs = np.array([5, 10]) and adding 'utime_pfair_pairwise' to the model_types list. Then, we can rerun all the cells for plotting in order to compare the results obtained with these strategy against the ones of the baseline recommendation algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
